---
title: The Easiest Way to Understand Diffusion Model Theory - II
published: 2025-06-18
tags: [Diffusion Model, Theory]
category: Diffusion Model Theory
draft: false
---

# The Denoising Diffusion Probabilistic Model (DDPMs)

Langevin dynamics can be used to generate samples from a distribution $p(\mathbf{x})$, given its score function $\mathbf{s}$. But its success hinges on two critical factors:

1. The method is highly sensitive to initialization - a poorly chosen $\mathbf{x}_0$ may trap the sampling process in local likelihood maxima, failing to explore the full distribution.
2. Inaccuracies in the score estimation, particularly near $\mathbf{x}_0$, can prevent convergence altogether.

These limitations led to the development of diffusion models, which use a unified initialization process: all samples are generated by gradually denoising pure Gaussian noise.

DDPMs [^Ho2020] are models that generate high-quality images from noise via a sequence of denoising steps. Denoting images as random variable $\mathbf{x}$ of the probabilistic density distribution $p(\mathbf{x})$, the DDPM aims to learn a model distribution that mimics the image distribution $p(\mathbf{x})$ and draw samples from it. The training and sampling of the DDPM utilize two diffusion processes: the forward and the backward diffusion process.

## The Forward Diffusion Process

The forward diffusion process of the DDPM provides necessary information to train a DDPM. It gradually adds noise to existing images $\mathbf{x}_0 \sim p(x)$ using the Ornstein-Uhlenbeck diffusion process (OU process) [^Uhlenbeck1930] within a finite time interval $t\in [0,T]$. The OU process is defined by the stochastic differential equation (SDE):

$$
d\mathbf{x}_t = -\frac{1}{2}\mathbf{x}_t dt + d\mathbf{W}_t \quad \text{(1.5 OU process Noise)}
$$

where:
- $t$ is the forward time of the diffusion process
- $\mathbf{x}_t$ is the noise contaminated image at time $t$
- $\mathbf{W}_t$ is a Brownian noise

The forward diffusion process has the standard Gaussian $\mathcal{N}(\mathbf{0},I)$ as its stationary distribution. Moreover, regardless of the initial distribution $p_0(\mathbf{x})$ of positions $\{\mathbf{x}_0^{(i)}\}_{i=1}^N$, their probability density $p_t(\mathbf{x})$ at time $t$ converges to $\mathcal{N}(\mathbf{x}|\mathbf{0}, I)$ as $t \to \infty$.

## The Backward Diffusion Process

The backward diffusion process is the conjugate of the forward process. While the forward process evolves $p_t$ toward $\mathcal{N}(\mathbf{0},I)$, the backward process reverses this evolution, restoring $\mathcal{N}(\mathbf{0},I)$ to $p_t$.

Consider the Langevin dynamics for $p_t(\mathbf{x})$ with a distinct time variable $\tau$:

$$
\begin{aligned}
d\mathbf{x}_\tau &= \mathbf{s}(\mathbf{x}_\tau, t) d\tau + \sqrt{2}d\mathbf{W}_\tau \\
&= \underbrace{-\frac{1}{2}\mathbf{x}_\tau d\tau + d\mathbf{W}_\tau^{(1)}}_{\text{Forward}} + \underbrace{\left(\frac{1}{2}\mathbf{x}_t + \mathbf{s}(\mathbf{x}_\tau, t)\right)d\tau + d\mathbf{W}_\tau^{(2)}}_{\text{Backward}} \quad \text{(langevin dynamics app2)}
\end{aligned}
$$

where $\mathbf{s}(\mathbf{x}, t) = \nabla_{\mathbf{x}}\log p_t(\mathbf{x})$ is the score function of $p_t(\mathbf{x})$.

The backward diffusion process is defined as:

$$
d\mathbf{x}_{t'} = \left(\frac{1}{2}\mathbf{x}_{t'} + \mathbf{s}(\mathbf{x}_{t'}, T-t')\right) dt' + d\mathbf{W}_{t'} \quad \text{(1.5 reverse diffusion process)}
$$

where $t' \in [0,T]$ is the backward time.

### Forward-Backward Duality

The forward and backward processes form a dual pair:
- Initialize $q_0(\mathbf{x}) = p_T(\mathbf{x})$
- Their evolution is related by $q_{t'}(\mathbf{x}) = p_{T-t'}(\mathbf{x})$
- For large $T$, $p_T(\mathbf{x}) \to \mathcal{N}(\mathbf{x}|\mathbf{0},I)$
- Thus $q_T(\mathbf{x}) = p_0(\mathbf{x})$

## Numerical Implementations

The forward OU process is discretized into the variance-preserving (VP) form [^Song2020]:

$$
\mathbf{x}_i = \sqrt{1-\beta_{i-1}}\mathbf{x}_{i-1} + \sqrt{\beta_{i-1}}\boldsymbol{\epsilon}_{i-1} \quad \text{(1.5 forward diffusion process sampling discrete, alt)}
$$

where:
- $i = 1,\cdots,n$ is the time step index
- $\beta_i$ is the step size
- $\boldsymbol{\epsilon}_i \sim \mathcal{N}(\mathbf{0},I)$

This can be rewritten as:

$$
\mathbf{x}_i = \sqrt{\bar{\alpha}_i}\mathbf{x}_0 + \sqrt{1-\bar{\alpha}_i}\bar{\boldsymbol{\epsilon}}_i \quad \text{(1.5 forward diffusion process sampling discrete, alt ss)}
$$

where $\bar{\alpha}_i = \prod_{j=0}^{i-1}(1-\beta_j)$.

For infinitesimal $\beta$, $\bar{\alpha}_i \to e^{-t_i}$.

The backward process is discretized as:

$$
\mathbf{x}_{i'+1} = \frac{\mathbf{x}_{i'} + \mathbf{s}(\mathbf{x}_{i'}, T-t'_{i'})\beta_{n-i'}}{\sqrt{1-\beta_{n-i'}}} + \sqrt{\beta_{n-i'}}\boldsymbol{\epsilon}_{i'} \quad \text{(1.5 backward diffusion process sampling discrete, alt)}
$$

## Training the Score Function

DDPMs minimize the denoising objective [^Ho2020]:

$$
L_{\text{denoise}}(\boldsymbol{\epsilon}_\theta) = \frac{1}{n}\sum_{i=1}^n \mathbb{E}_{\mathbf{x}_0 \sim p_0(\mathbf{x})} \mathbb{E}_{\bar{\boldsymbol{\epsilon}}_i\sim\mathcal{N}(\mathbf{0},I)} \|\bar{\boldsymbol{\epsilon}}_i - \boldsymbol{\epsilon}_\theta(\mathbf{x}_i, t_i)\|_2^2 \quad \text{(1.6 DDPM objective)}
$$

The noise is related to the score function:

$$
\bar{\boldsymbol{\epsilon}}_i = -\sqrt{1-\bar{\alpha}_i}\mathbf{s}(\mathbf{x}_i|\mathbf{x}_0, t_i) \quad \text{(score and eps)}
$$

Thus, the denoising network approximates a scaled score function [^Yang2022]:

$$
\boldsymbol{\epsilon}_\theta(\mathbf{x}, t_i) \approx -\sqrt{1-\bar{\alpha}_i}\mathbf{s}(\mathbf{x}, t_i) \quad \text{(1.6 DDPM result score eps)}
$$

## References

[^Ho2020]: Ho, J., Jain, A., & Abbeel, P. (2020). Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33.
[^Uhlenbeck1930]: Uhlenbeck, G. E., & Ornstein, L. S. (1930). On the theory of the Brownian motion. Physical Review, 36(5), 823.
[^Anderson1982]: Anderson, B. D. (1982). Reverse-time diffusion equation models. Stochastic Processes and their Applications, 12(3), 313-326.
[^Song2020]: Song, Y., & Ermon, S. (2020). Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456.
[^Yang2022]: Yang, L., Zhang, Z., Song, Y., et al. (2022). Diffusion models: A comprehensive survey of methods and applications. arXiv preprint arXiv:2209.00796.

---
