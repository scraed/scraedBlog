---
title: The Easiest Way to Understand Diffusion Model Theory - II
published: 2025-06-18
tags: [Diffusion Model, Theory]
category: Diffusion Model Theory
draft: false
---

# The Denoising Diffusion Probabilistic Model (DDPMs) {#AppB}

Langevin dynamics can be used to generate samples from a distribution $p(\mathbf{x})$, given its score function $\mathbf{s}$. But its success hinges on two critical factors. First, the method is highly sensitive to initialization - a poorly chosen $\mathbf{x}_0$ may trap the sampling process in local likelihood maxima, failing to explore the full distribution. Second, inaccuracies in the score estimation, particularly near $\mathbf{x}_0$, can prevent convergence altogether. These limitations led to the development of diffusion models, which use a unified initialization process: all samples are generated by gradually denoising pure Gaussian noise.  

DDPMs [@Ho2020DenoisingDP] are models that generate high-quality images from noise via a sequence of denoising steps. Denoting images as random variable $\mathbf{x}$ of the probabilistic density distribution $p(\mathbf{x})$, the DDPM aims to learn a model distribution that mimics the image distribution $p(\mathbf{x})$ and draw samples from it. The training and sampling of the DDPM utilize two diffusion process: the forward and the backward diffusion process. 

## The forward diffusion process

The forward diffusion process of the DDPM provides necessary information to train a DDPM. It gradually adds noise to existing images $\mathbf{x}_0 \sim p(x)$ using the Ornstein-Uhlenbeck diffusion process (OU process) [@Uhlenbeck1930OnTT] within a finite time interval $t\in [0,T]$. The OU process is defined by the stochastic differential equation (SDE):

\begin{equation} \label{1.5 OU process Noise}
d \mathbf{x}_t = - \frac{1}{2} \mathbf{x}_t dt + d\mathbf{W}_t,
\end{equation}

where $t$ is the forward time of the diffusion process, $\mathbf{x}_t$ is the noise contaminated image at time $t$, and $\mathbf{W}_t$ is a Brownian noise.

The forward diffusion process has the standard Gaussian $\mathcal{N}(\mathbf{0},I)$ as its stationary distribution. Moreover, regardless of the initial distribution $p_0(\mathbf{x})$ of positions $\{\mathbf{x}_0^{(i)}\}_{i=1}^N$, their probability density $p_t(\mathbf{x})$ at time $t$ converges to $\mathcal{N}(\mathbf{x}| \mathbf{0}, I)$ as $t \to \infty$.  

## The backward diffusion process

The backward diffusion process is the conjugate of the forward process. While the forward process evolves $p_t$ toward $\mathcal{N}(\mathbf{0},I)$, the backward process reverses this evolution, restoring $\mathcal{N}(\mathbf{0},I)$ to $p_t$. To derive it, we know from previous section that Langevin dynamics \eqref{langevin dynamics app1} acts as an "identity" operation on a distribution. Thus, the composition of forward and backward processes, at time $t$, must yield the Langevin dynamics for $p_t(\mathbf{x})$.

To formalize this, consider the Langevin dynamics for $p_t(\mathbf{x})$ with a distinct time variable $\tau$, distinguished from the forward diffusion time $t$. This dynamics can be decomposed into forward and backward components as follows:  

\begin{equation} \label{langevin dynamics app2}  
\begin{split}  
d\mathbf{x}_\tau &= \mathbf{s}(\mathbf{x}_\tau, t) d\tau + \sqrt{2}\, d\mathbf{W}_\tau, \\
&= \underbrace{-\frac{1}{2} \mathbf{x}_\tau d\tau + d\mathbf{W}_\tau^{(1)}}_{\text{Forward}} + \underbrace{ \left( \frac{1}{2} \mathbf{x}_t + \mathbf{s}(\mathbf{x}_\tau, t) \right)d\tau + d\mathbf{W}_\tau^{(2)}}_{\text{Backward} },  
\end{split}  
\end{equation}  

where $\mathbf{s}(\mathbf{x}, t) = \nabla_{\mathbf{x}} \log p_t(\mathbf{x})$ is the score function of $p_t(\mathbf{x})$. The "Forward" part corresponds to the forward diffusion process \eqref{1.5 OU process Noise}, effectively increasing the forward diffusion time $t$ by $d\tau$, bringing the distribution to $p_{t + d\tau}(\mathbf{x})$. Since the forward and backward components combine to form an "identity" operation, the "Backward" part in \eqref{langevin dynamics app2} must reverse the forward processâ€”decreasing the forward diffusion time $t$ by $d\tau$ and restoring the distribution back to $p_t(\mathbf{x})$.

Now we can define the backward process according to the backward part in \eqref{langevin dynamics app2}, and a backward diffusion time $t'$ different from the forward diffusion time $t$:

\begin{equation} \label{1.5 reverse diffusion process 1}
d\mathbf{x}_{t'} = \left( \frac{1}{2} \mathbf{x}_{t'}+ \mathbf{s}(\mathbf{x}_{t'}, t) \right) dt' + d\mathbf{W}_{t'}.
\end{equation}

It remains to determine the relation between the forward diffusion time $t$ and backward diffusion time $t'$. Since $dt'$ is interpreted as "decrease" the forward diffusion time $t$, we have 

\begin{equation} \label{dt dt' relation}
    dt = -dt'
\end{equation}

which means the backward diffusion time is the inverse of the forward. To make $t'$ lies in the same range $[0, T]$ of the forward diffusion time, we define $t = T - t'$. In this notation, the backward diffusion process [@Anderson1982ReversetimeDE] is

\begin{equation} \label{1.5 reverse diffusion process}
d\mathbf{x}_{t'} = \left( \frac{1}{2} \mathbf{x}_{t'}+ \mathbf{s}(\mathbf{x}_{t'}, T-t') \right) dt' + d\mathbf{W}_{t'},
\end{equation} 

where $t' \in [0,T]$ is the backward time, $\mathbf{s}(\mathbf{x}, t) = \nabla_{\mathbf{x}} \log p_t(\mathbf{x})$ is the score function of the density of $\mathbf{x}_{t}$ in the forward process.

### Forward-Backward Duality

The forward and backward processes form a dual pair, advancing the time $t'$ means receding time $t$ by the same amount. We define the densities of $\mathbf{x}_t$ (forward) as $p_t(\mathbf{x})$, the densities of $\mathbf{x}_{t'}$ (backward) as $q_{t'}(\mathbf{x})$. If we initialize

\begin{equation}  
    q_0(\mathbf{x}) = p_T(\mathbf{x}),  
\end{equation}  

then their evolution are related by  

\begin{equation}  
    q_{t'}(\mathbf{x}) = p_{T-t'}(\mathbf{x}) 
\end{equation}  

For large $T$, $p_T(\mathbf{x})$ converges to $\mathcal{N}(\mathbf{x}|\mathbf{0},I)$. Thus, the backward process starts at $t'=0$ with $\mathcal{N}(\mathbf{0},I)$ and, after evolving to $t'=T$, generates samples from the data distribution:  

\begin{equation}  
    q_T(\mathbf{x}) = p_0(\mathbf{x}).  
\end{equation}  

This establishes an exact correspondence between the forward diffusion process and the backward diffusion process.

### Numerical Implementations

In practice, the forward OU process \eqref{1.5 OU process Noise} is numerically discretized into the variance-preserving (VP) form [@Song2020ScoreBasedGM]:

\begin{equation} \label{1.5 forward diffusion process sampling discrete, alt}
   \mathbf{x}_{i} =  \sqrt{1-\beta_{i-1}} \mathbf{x}_{i-1}  + \sqrt{beta_{i-1}}\boldsymbol{\epsilon}_{i-1},
\end{equation}

where $i = 1,\cdots,n$ is the number of the time step, $\beta_i$ is the step size of each time step, $\mathbf{x}_i$ is image at $i$th time step with time $t_i = \sum_{j=0}^{i-1} \beta_j$, $\boldsymbol{\epsilon}_{i}$ is standard Gaussian random variable. The time step size usually takes the form $\beta_i = \frac{i(b_2 - b_1)}{n-1} + b_1$ where $b_1 = 10^{-4}$ and $b_2 = 0.02$. Note that our interpretation of $\beta$ differs from that in [@Song2020ScoreBasedGM], treating $\beta$ as a varying time-step size to solve the autonomous SDE \eqref{1.5 OU process Noise} instead of a time-dependent SDE. Our interpretation holds as long as every $\beta_i^2$ is negligible and greatly simplifies future analysis. The discretized OU process \eqref{1.5 forward diffusion process sampling discrete, alt} adds a small amount of Gaussian noise to the image at each time step $i$, gradually contaminating the image until $\mathbf{x}_n \sim \mathcal{N}(\mathbf{0},I)$.

Training a DDPM aims to recover the original image $x_0$ from one of its contaminated versions $x_i$. In this case \eqref{1.5 forward diffusion process sampling discrete, alt} could be rewritten into the **forward diffusion process**

\begin{equation} \label{1.5 forward diffusion process sampling discrete, alt ss}
   \mathbf{x}_{i} 
=\sqrt{\bar{\alpha}_i}\mathbf{x}_0 + \sqrt{1-\bar{\alpha}_i}\bar{\boldsymbol{\epsilon}}_i; \quad 1\le i \le n,
\end{equation}

where $\bar{\alpha}_i = \prod_{j=0}^{i-1} (1-\beta_j)$ is the weight of contamination and $\bar{\boldsymbol{\epsilon}}_i$ is a standard Gaussian random noise to be removed.

An useful property we shall exploit later is that for **infinitesimal** time steps $\beta$, the contamination weight $\bar{\alpha}_i$ is the exponential of the diffusion time $t_i$

\begin{equation} \label{alpha bar limit}
  \lim_{\max_j \beta_j \xrightarrow[]{}0} \bar{\alpha}_i  \xrightarrow[]{} e^{-t_i}.
\end{equation}

The **backward diffusion process** is used to sample from the DDPM by removing the noise of an image step by step. It is the time reversed version of the OU process, starting at $x_{0'} \sim \mathcal{N}(\mathbf{x}|\mathbf{0}, I)$, using the reverse of the OU process \eqref{1.5 reverse diffusion process}. In practice, the backward diffusion process is discretized into 

\begin{equation} \label{1.5 backward diffusion process sampling discrete, alt}
   \mathbf{x}_{i'+1} = \frac{\mathbf{x}_{i'} + \mathbf{s}(\mathbf{x}_{i'}, T-t'_{i'}) \beta_{n-i'}}{\sqrt{1-\beta_{n-i'}} }  + \sqrt{\beta_{n-i'}}\boldsymbol{\epsilon}_{i'},
\end{equation}

where $i' = 0, \cdots, n$ is the number of the backward time step, $\mathbf{x}_{i'}$ is image at $i'$th backward time step with time $t_{i'}' = \sum_{j=0}^{i'-1} \beta_{n-1-j} = T - t_{n-i'}$. This discretization is consistent with \eqref{1.5 reverse diffusion process} as long as $\beta_i^2$ are negligible. The score function $\mathbf{s}(\mathbf{x}_{i'}, T-t'_{i'})$ is generally modeled by a neural network and trained with a denoising objective. 

### Training the score function

Training the score function requires a training objective. We will show that the score function could be trained with a denoising objective. 

DDPM is trained to removes the noise $\bar{\boldsymbol{\epsilon}}_i$ from $\mathbf{x}_i$ in \eqref{1.5 forward diffusion process sampling discrete, alt ss}, by training a denoising neural network $\boldsymbol{\epsilon}_\theta( \mathbf{x}, t_i )$ to predict and remove the noise $\bar{\boldsymbol{\epsilon}}_i $. This means that DDPM minimizes the **denoising objective** [@Ho2020DenoisingDP]:

\begin{equation} \label{1.6 DDPM objective}
\begin{split}
     L_{denoise}(\boldsymbol{\epsilon}_\theta) &=\frac{1}{n}\sum_{i=1}^n \mathbf{E}_{\mathbf{x}_0 \sim p_0(\mathbf{x})}  \mathbf{E}_{\bar{\boldsymbol{\epsilon}}_i\sim \mathcal{N}(\mathbf{0},I)}\| \bar{\boldsymbol{\epsilon}}_i  -  \boldsymbol{\epsilon}_\theta( \bold{x}_i, t_i  )\|_2^2.
\end{split}
\end{equation}

Now we show that $\boldsymbol{\epsilon}_\theta$ trained with the above objective is proportional to the score function $\mathbf{s}$. Note that the \eqref{1.5 forward diffusion process sampling discrete, alt ss} tells us that the distribution of $\mathbf{x}_i$ given $\mathbf{x}_0$ is a Gaussian distribution

\begin{equation} \label{condition distribution App1}
p(\mathbf{x}_i | \mathbf{x}_0) = \mathcal{N}(\mathbf{x}_i|\sqrt{\bar{\alpha}_i}\mathbf{x}_0, (1-\bar{\alpha}_i) I),
\end{equation}

and the noise $\bar{\boldsymbol{\epsilon}}_i$ in \eqref{1.5 forward diffusion process sampling discrete, alt ss} is directly proportional to the score function

\begin{equation} \label{score and eps}
\bar{\boldsymbol{\epsilon}}_i = -\sqrt{1-\bar{\alpha}_i}  \mathbf{s}(\mathbf{x}_i | \mathbf{x}_0, t_i),
\end{equation}

where $\mathbf{s}(\mathbf{x}_i | \mathbf{x}_0, t_i)=\nabla_{\mathbf{x}_i} \log p(\mathbf{x}_i | \mathbf{x}_0)$ is the score of the conditional probability density $p(\mathbf{x}_i | \mathbf{x}_0)$ at $\mathbf{x}_i$. 

The \eqref{score and eps} is an important property. It tells us that the noise $\bar{\boldsymbol{\epsilon}}_i$ is directly related to a conditional score function. This conditional score function is connected to the score function $\mathbf{s}(\mathbf{x}, t)$ through the following equation:

\begin{equation} \label{denoise equivalence}
    \mathbf{E}_{\mathbf{x}_i \sim p_{t_i}(\mathbf{x})} f(\mathbf{x}_i) \mathbf{s}(\mathbf{x}, t_i) = \mathbf{E}_{\mathbf{x}_0 \sim p_0(\mathbf{x})}  \mathbf{E}_{\mathbf{x}_i\sim p(\mathbf{x}_i | \mathbf{x}_0)}  f(\mathbf{x}_i) \mathbf{s} (\mathbf{x}_i | \mathbf{x}_0)
\end{equation}

where $f$ is an arbitrary function and $\mathbf{s}(\mathbf{x}, t) =\nabla_{\mathbf{x}} \log p_t(\mathbf{x})$ is the score function of the probability density of $\mathbf{x}_t$.

Substituting \eqref{score and eps} into \eqref{1.6 DDPM objective} and utilizing \eqref{denoise equivalence}, we could derive that \eqref{1.6 DDPM objective} is equivalent to a denoising score matching objective  

\begin{equation}
\label{1.6 DDPM objective s matching}
\begin{split}
     L_{denoise}(\boldsymbol{\epsilon}_\theta) &=\frac{1}{n}\sum_{i=1}^{n}   \mathbf{E}_{\mathbf{x_i}\sim p_{t_i}(\mathbf{x})} \| \sqrt{1-\bar{\alpha}_i}  \mathbf{s}(\mathbf{x}_i, t_i)  + \boldsymbol{\epsilon}_\theta( \bold{x}_i, t_i  )\|_2^2,
\end{split}
\end{equation}

This objectives says that the denoising neural network $\boldsymbol{\epsilon}_\theta( \mathbf{x}, t_i )$ is trained to approximate a scaled score function $\boldsymbol{\epsilon}( \mathbf{x}, t_i )$ [@Yang2022DiffusionMA]

\begin{equation} \label{1.6 DDPM result score eps}
\boldsymbol{\epsilon}_\theta( \mathbf{x}, t_i  ) \approx  -\sqrt{1-\bar{\alpha}_i}\mathbf{s}(\mathbf{x}, t_i).
\end{equation}

Therefore the denoising neural network is actually a scaled estimate of the score function $\mathbf{s}(\mathbf{x}, t)$, hence could be inserted into the backward sampling process \eqref{1.5 backward diffusion process sampling discrete, alt} to generate images.

## References

- @Ho2020DenoisingDP: Ho, J., Jain, A., & Abbeel, P. (2020). Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33.
- @Uhlenbeck1930OnTT: Uhlenbeck, G. E., & Ornstein, L. S. (1930). On the theory of the Brownian motion. Physical Review, 36(5), 823.
- @Anderson1982ReversetimeDE: Anderson, B. D. (1982). Reverse-time diffusion equation models. Stochastic Processes and their Applications, 12(3), 313-326.
- @Song2020ScoreBasedGM: Song, Y., & Ermon, S. (2020). Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456.
- @Yang2022DiffusionMA: Yang, L., Zhang, Z., Song, Y., et al. (2022). Diffusion models: A comprehensive survey of methods and applications. arXiv preprint arXiv:2209.00796.

---
